{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Required Modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Pre-Processed Train / Test Data\n",
    "\n",
    "train_df = pd.read_csv('Train-Data.csv')\n",
    "test_df = pd.read_csv('Test-Data.csv')\n",
    "\n",
    "print(f'Train Length: {len(train_df)}, Test Length: {len(test_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare Data For ML input\n",
    "trainx = train_df.iloc[:,:-1].values.tolist()\n",
    "trainy = train_df['label'].tolist()\n",
    "\n",
    "testx = test_df.iloc[:,:-1].values.tolist()\n",
    "testy = test_df['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validate Correct Input Shapes\n",
    "print(f'Trainx: {len(trainx)}, Trainy: {len(trainy)}')\n",
    "print(f'Testx: {len(testx)}, Testy: {len(testy)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode Classification Labels\n",
    "le = LabelEncoder()\n",
    "\n",
    "le.fit(trainy)\n",
    "\n",
    "trainy = le.transform(trainy)\n",
    "testy = le.transform(testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vector Machine\n",
    "SVM_Params_Grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "SVM = SVC()\n",
    "SVM_grid_search = GridSearchCV(SVM, SVM_Params_Grid, cv=3)\n",
    "SVM_grid_search.fit(trainx, trainy)\n",
    "SVM_preds = SVM_grid_search.predict(testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Knn Classifier\n",
    "Knn_Params_grid = {\n",
    "    'n_neighbors': list(range(1, 31)),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "}\n",
    "\n",
    "Knn = KNeighborsClassifier()\n",
    "Knn_grid_search = GridSearchCV(Knn, Knn_Params_grid, cv=3)\n",
    "Knn_grid_search.fit(trainx, trainy)\n",
    "Knn_preds = Knn_grid_search.predict(testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Classifier\n",
    "rnd_forest_Params_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rnd_forest = RandomForestClassifier()\n",
    "rnd_forest_grid_search = GridSearchCV(rnd_forest, rnd_forest_Params_grid, cv=3)\n",
    "rnd_forest_grid_search.fit(trainx, trainy)\n",
    "rnd_forest_preds = rnd_forest_grid_search.predict(testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to generate performance metrics\n",
    "def get_performance_metrics(preds):\n",
    "    metrics = {}\n",
    "    metrics['acc'] = accuracy_score(testy, preds)\n",
    "    metrics['prec'] = precision_score(testy, preds, average='macro')\n",
    "    metrics['rec'] = recall_score(testy, preds, average='macro')\n",
    "    metrics['f1'] = f1_score(testy, preds, average='macro')\n",
    "    metrics['cm'] = confusion_matrix(testy, preds)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate performance metrics\n",
    "SVM_Metrics = get_performance_metrics(SVM_preds)\n",
    "Knn_Metrics = get_performance_metrics(Knn_preds)\n",
    "rnd_forest_Metrics = get_performance_metrics(rnd_forest_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display Performanec Metrics\n",
    "print(\"SVM Metrics:\", {k:round(v, 3) for k,v in SVM_Metrics.items() if k != 'cm'})\n",
    "print(\"Knn Metrics:\", {k:round(v, 3) for k,v in Knn_Metrics.items() if k != 'cm'})\n",
    "print(\"random Forest Metrics:\", {k:round(v, 3) for k,v in rnd_forest_Metrics.items() if k != 'cm'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualise Metrics\n",
    "SVM_Dict = {k:round(v, 3) for k,v in SVM_Metrics.items() if k != 'cm'}\n",
    "Knn_Dict = {k:round(v, 3) for k,v in Knn_Metrics.items() if k != 'cm'}\n",
    "Rnd_Forest_Dict = {k:round(v, 3) for k,v in rnd_forest_Metrics.items() if k != 'cm'}\n",
    "\n",
    "labels = list(SVM_Dict.keys())\n",
    "SVM_vals = list(SVM_Dict.values())\n",
    "Knn_vals = list(Knn_Dict.values())\n",
    "rnd_forest_vals = list(Rnd_Forest_Dict.values())\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "width = 0.3\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "rects1 = ax.bar(x - width, SVM_vals, width, label='SVM', alpha=0.7)\n",
    "rects2 = ax.bar(x, Knn_vals, width, label='K-NN', alpha=0.7)\n",
    "rects3 = ax.bar(x + width, rnd_forest_vals, width, label='Rnd Forest', alpha=0.7)\n",
    "\n",
    "ax.set_ylim(0.95, 1.0)\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('Values by metric and model')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualise Confusion Matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "cms = [SVM_Metrics['cm'], Knn_Metrics['cm'], rnd_forest_Metrics['cm']]\n",
    "titles = ['SVM', 'K-NN', 'Random Forest']\n",
    "\n",
    "for ax, cm, title in zip(axes, cms, titles):\n",
    "\n",
    "    cax = ax.matshow(cm, cmap=plt.cm.Blues)\n",
    "    plt.colorbar(cax, ax=ax)\n",
    "    ax.set_title(title, pad=20)\n",
    "    ax.set_xticks(np.arange(len(le.classes_)))\n",
    "    ax.set_yticks(np.arange(len(le.classes_)))\n",
    "    ax.set_xticklabels(le.classes_, rotation=90)\n",
    "    ax.set_yticklabels(le.classes_)\n",
    "    \n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, str(cm[i, j]), ha='center', va='center', color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
